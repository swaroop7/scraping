{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from nltk.stem.porter import *\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "os.chdir('/Users/krothaps/Desktop/hackathon/dataset/tim_first_level_final/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inlcude_only_nouns(lines):\n",
    "    # function to test if something is a noun\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "\n",
    "    tokenized = nltk.word_tokenize(lines)\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "\n",
    "    return ' '.join(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_file_data(file_name):\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "    \n",
    "    file_text_data = ''\n",
    "    for line in content:\n",
    "        line = line.decode('utf-8')\n",
    "        text_data = \"\\t\".join(line.split(\"\\t\", 2)[2:])\n",
    "        # include only nouns\n",
    "        text_data = inlcude_only_nouns(text_data)\n",
    "        # remove words which are 3 characters or less\n",
    "        text_data = shortword.sub('', text_data)\n",
    "        # remove urls\n",
    "        text_data = re.sub(r'http\\S+', '', text_data)\n",
    "        text_data = re.sub(r'https\\S+', '', text_data)\n",
    "#         text_data = ''.join([i for i in text_data if not i.isdigit()])\n",
    "        # lemmatization\n",
    "#         text_data = ' '.join([lmtzr.lemmatize(word) for word in text_data.split()])\n",
    "        # stemming        \n",
    "#         text_data = ' '.join([stemmer.stem(word) for word in text_data.split()])\n",
    "        # spell correction\n",
    "#         text_data = text_data.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').replace('  ', ' ')\n",
    "        text_data = text_data.lower()\n",
    "        #include only numerical data\n",
    "        text_data = re.sub(r\"[^a-z]+\", ' ', text_data)\n",
    "        \n",
    "        file_text_data += text_data\n",
    "        \n",
    "    return filename+'\\t'+file_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\t2\n",
      "20\t14\n",
      "30\t10\n",
      "40\t7\n",
      "50\t7\n",
      "60\t5\n",
      "70\t7\n",
      "80\t0\n",
      "90\t4\n",
      "100\t1\n",
      "110\t7\n",
      "120\t0\n",
      "130\t8\n",
      "140\t0\n",
      "150\t6\n",
      "160\t14\n",
      "170\t10\n",
      "180\t0\n",
      "190\t0\n",
      "200\t2\n",
      "210\t1\n",
      "220\t1\n",
      "230\t8\n",
      "240\t0\n",
      "250\t12\n",
      "260\t6\n",
      "270\t11\n",
      "280\t1\n",
      "290\t0\n",
      "300\t0\n",
      "310\t3\n",
      "320\t4\n",
      "330\t0\n",
      "340\t27\n",
      "350\t0\n",
      "360\t0\n",
      "370\t0\n",
      "380\t0\n",
      "390\t0\n",
      "400\t0\n",
      "410\t1\n",
      "420\t3\n",
      "430\t8\n",
      "440\t0\n",
      "450\t1\n",
      "460\t6\n",
      "470\t0\n",
      "480\t5\n",
      "490\t3\n",
      "500\t4\n",
      "510\t0\n",
      "520\t8\n",
      "530\t1\n",
      "540\t1\n",
      "550\t2\n",
      "560\t10\n",
      "570\t2\n",
      "580\t0\n",
      "590\t1\n",
      "600\t0\n",
      "610\t0\n",
      "620\t1\n",
      "630\t5\n",
      "640\t3\n",
      "650\t9\n",
      "660\t0\n",
      "670\t0\n",
      "680\t0\n",
      "690\t0\n",
      "700\t0\n",
      "710\t0\n",
      "720\t0\n",
      "730\t3\n",
      "740\t3\n",
      "750\t3\n",
      "760\t0\n",
      "770\t0\n",
      "780\t0\n",
      "790\t1\n",
      "800\t0\n",
      "810\t5\n",
      "820\t3\n",
      "830\t0\n",
      "840\t0\n",
      "850\t7\n",
      "860\t2\n",
      "870\t2\n",
      "880\t5\n",
      "890\t10\n",
      "900\t6\n",
      "910\t0\n",
      "920\t0\n",
      "930\t0\n",
      "940\t22\n",
      "950\t1\n",
      "960\t5\n",
      "970\t3\n",
      "980\t253\n",
      "--- Finished parsing text in 6025.00171018 seconds---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "output = open('/Users/krothaps/Desktop/hackathon/dataset/test_final.txt', 'w')\n",
    "\n",
    "count = 0\n",
    "\n",
    "for filename in glob.glob(os.path.join('*.txt')):\n",
    "    count += 1\n",
    "    start1_time = time.time()\n",
    "    file_data = parse_file_data(filename)\n",
    "    output.write(file_data+'\\n')\n",
    "    if count % 10 == 0 :\n",
    "        print str(count)+'\\t'+str(int(time.time() - start1_time))\n",
    "    \n",
    "output.close()\n",
    "\n",
    "print \"--- Finished parsing text in \" + str(time.time() - start_time)+' seconds---'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the text data to TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finished parsing text in 6025.03002501 seconds---\n"
     ]
    }
   ],
   "source": [
    "print \"--- Finished parsing text in \" + str(time.time() - start_time)+' seconds---'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(978, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/krothaps/Desktop/hackathon/dataset/test_final.txt', delimiter='\\t', names=['id','text'])\n",
    "\n",
    "print data.shape\n",
    "\n",
    "# tfv = TfidfVectorizer(min_df=250, \n",
    "#             strip_accents='unicode', analyzer='word',\n",
    "#             stop_words = 'english')\n",
    "\n",
    "tfv = CountVectorizer(max_df=0.95, min_df=5,\n",
    "                                stop_words='english')\n",
    "\n",
    "#   ngram_range=(1, 3), token_pattern=r'\\w{1,}', use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "\n",
    "df = data['text'].fillna('')\n",
    "\n",
    "tfv.fit(df)\n",
    "data_tfv =  tfv.transform(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(978, 2)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28835\n"
     ]
    }
   ],
   "source": [
    "result =  pd.DataFrame(data_tfv.toarray())\n",
    "\n",
    "# result = result.drop(['id'], axis=1)\n",
    "\n",
    "result.columns = tfv.get_feature_names()\n",
    "print len(result.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(978, 28835)\n",
      "done\n",
      "(978, 3695)\n"
     ]
    }
   ],
   "source": [
    "print result.shape\n",
    "\n",
    "result_filtered = result\n",
    "sum_colums = []\n",
    "\n",
    "for column in result:\n",
    "    if sum(result[column]) < 500:\n",
    "        sum_colums.append(column)\n",
    "        \n",
    "print 'done'        \n",
    "result_filtered = result_filtered.drop(sum_colums, axis=1)\n",
    "\n",
    "print result_filtered.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result_filtered.to_csv('count_vectorizer_min_500.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'id', u'text'], dtype='object')\n",
      "(978, 3696)\n"
     ]
    }
   ],
   "source": [
    "print data.columns\n",
    "\n",
    "result_filtered_final = pd.concat([pd.DataFrame(data['id']), result_filtered], axis=1)\n",
    "\n",
    "print result_filtered_final.shape\n",
    "result_filtered_final.to_csv('count_vectorizer_min_500_v1.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = final[['id','computing']]\n",
    "\n",
    "# print df1.shape\n",
    "\n",
    "# df1.to_csv('two_column_output.txt', sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krothaps/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# Distance Matrix \n",
    "\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "\n",
    "print type(data)\n",
    "\n",
    "tfidf_scores = final.drop(['id'], axis=1)\n",
    "\n",
    "tfidf_scores_list = tfidf_scores.values.tolist()\n",
    "\n",
    "id_list = final.ix[:,0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(979, 28837)\n",
      "(979, 2)\n",
      "(979, 28839)\n",
      "(979, 28837)\n"
     ]
    }
   ],
   "source": [
    "print result.shape\n",
    "print data.shape\n",
    "\n",
    "\n",
    "final = pd.concat([data, result], axis=1)\n",
    "\n",
    "print final.shape\n",
    "\n",
    "final = final.drop(['text'], axis=1)\n",
    "\n",
    "print final.shape\n",
    "\n",
    "sl = pd.Series(school_list)\n",
    "\n",
    "final['school_name'] = sl.values\n",
    "\n",
    "final.to_csv('sample_final_count_vectorizer.txt', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analys\n",
      "analyses\n",
      "analysis\n",
      "analyst\n",
      "analysts\n",
      "analytic\n",
      "analytical\n",
      "analytics\n",
      "analyze\n",
      "analyzer\n",
      "analyzes\n",
      "analyzing\n",
      "psychoanalysis\n"
     ]
    }
   ],
   "source": [
    "feature = 'analy'\n",
    "\n",
    "for x in result.columns:\n",
    "    if feature in x:\n",
    "        print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       school_name  applied\n",
      "491                     kean college of new jersey     4183\n",
      "537                               le moyne college     1224\n",
      "512                  western new mexico university      629\n",
      "221                      lewis-clark state college      452\n",
      "237               illinois institute of technology      381\n",
      "174                south florida community college      324\n",
      "762                        dakota state university      321\n",
      "798           texas a & m international university      309\n",
      "450                       missouri baptist college      243\n",
      "666             oklahoma state university-okmulgee      242\n",
      "790                texas a & m university-commerce      241\n",
      "858  virginia polytechnic institute and state univ      220\n",
      "464                          washington university      197\n",
      "217              university of hawaii at west oahu      162\n",
      "391                                   hope college      156\n",
      "874                     highline community college      147\n",
      "438          university of mississippi main campus      127\n",
      "17                  university of alaska fairbanks      123\n",
      "738            new england institute of technology      114\n",
      "159                            university of miami      113\n",
      "685                             arcadia university      112\n",
      "236               university of illinois at urbana      109\n",
      "258                              butler university       98\n",
      "750                      francis marion university       97\n",
      "574                             buffalo state suny       88\n",
      "403                           northwood university       80\n",
      "80                       san jose state university       76\n",
      "570                             suny at binghamton       74\n",
      "846                        george mason university       73\n",
      "23           university of arkansas at little rock       72\n"
     ]
    }
   ],
   "source": [
    "# count vectorizer data\n",
    "\n",
    "feature = 'applied'\n",
    "\n",
    "# count_vectorizer_final = final\n",
    "\n",
    "eda = count_vectorizer_final[['school_name', feature]]\n",
    "\n",
    "print eda.sort_values(by=feature, ascending=False).head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17287, 2)\n",
      "(979, 979)\n"
     ]
    }
   ],
   "source": [
    "df_uid_school_name = pd.read_csv('/Users/krothaps/Desktop/hackathon/presentation/uid_school_name.txt',  names = ['uid', 'school_name'])\n",
    "\n",
    "# convert to lower case \n",
    "df_uid_school_name['school_name']  =  df_uid_school_name['school_name'].str.lower()\n",
    "# remove duplicates\n",
    "df_uid_school_name = df_uid_school_name.drop_duplicates()\n",
    "\n",
    "\n",
    "print df_uid_school_name.shape\n",
    "\n",
    "# print df_uid_school_name.dtypes\n",
    "\n",
    "df_uid_school_name['uid'] = df_uid_school_name['uid'].astype(str)\n",
    "\n",
    "school_list = []\n",
    "\n",
    "\n",
    "for x in id_list:\n",
    "    s = x.replace('.txt', '')\n",
    "    try:\n",
    "        matching_row = df_uid_school_name.loc[df_uid_school_name['uid'] == s].iloc[0]\n",
    "        school_list.append(matching_row['school_name'])\n",
    "    except:\n",
    "        school_list.append('missing')\n",
    "\n",
    "df = pd.DataFrame(tfidf_scores_list, columns=tfidf_scores.columns.tolist(), index=school_list)        \n",
    "        \n",
    "distance_matrix = pd.DataFrame(distance_matrix(df.values, df.values), index=df.index, columns=df.index)\n",
    "\n",
    "print distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix.to_csv('/Users/krothaps/Desktop/hackathon/presentation/distance_matrix2.txt', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
