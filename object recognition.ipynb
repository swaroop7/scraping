{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image                                                                                \n",
    "\n",
    "os.chdir('/Users/krothaps/Desktop/hackathon/demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[188  25   0]\n",
      "  [189  25   0]\n",
      "  [189  25   0]\n",
      "  ..., \n",
      "  [180  23   0]\n",
      "  [179  23   0]\n",
      "  [171  21   0]]\n",
      "\n",
      " [[201  27   0]\n",
      "  [202  27   0]\n",
      "  [202  27   0]\n",
      "  ..., \n",
      "  [193  26   0]\n",
      "  [192  25   0]\n",
      "  [183  24   0]]\n",
      "\n",
      " [[212  29   0]\n",
      "  [213  30   0]\n",
      "  [214  30   0]\n",
      "  ..., \n",
      "  [204  28   0]\n",
      "  [202  27   0]\n",
      "  [192  25   0]]\n",
      "\n",
      " ..., \n",
      " [[185  24   0]\n",
      "  [187  24   0]\n",
      "  [187  24   0]\n",
      "  ..., \n",
      "  [169  21   0]\n",
      "  [165  20   0]\n",
      "  [157  19   0]]\n",
      "\n",
      " [[161  20   0]\n",
      "  [163  20   0]\n",
      "  [163  20   0]\n",
      "  ..., \n",
      "  [145  16   0]\n",
      "  [142  16   0]\n",
      "  [135  14   0]]\n",
      "\n",
      " [[152  22   2]\n",
      "  [153  22   1]\n",
      "  [153  22   1]\n",
      "  ..., \n",
      "  [137  20   5]\n",
      "  [135  19   5]\n",
      "  [129  19   7]]]\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"image.png\")\n",
    "copy = image.copy()\n",
    "print image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray', gray)\n",
    "cv2.waitKey(0)\n",
    " \n",
    "edged = cv2.Canny(gray, 10, 250)\n",
    "cv2.imshow('Edged', edged)\n",
    "cv2.waitKey(0)\n",
    " \n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    " \n",
    "dilation = cv2.dilate(edged, kernel, iterations=1)\n",
    "cv2.imshow('Dilation', dilation)\n",
    "cv2.waitKey(0)\n",
    " \n",
    "closing = cv2.morphologyEx(dilation, cv2.MORPH_CLOSE, kernel)\n",
    "cv2.imshow('Closing', closing)\n",
    "cv2.waitKey(0)\n",
    " \n",
    "# (image, cnts, hiers) = cv2.findContours(closing, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    " \n",
    "# cont = cv2.drawContours(copy, cnts, -1, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "# cv2.imshow('Contours', cont)\n",
    "# cv2.waitKey(0)\n",
    " \n",
    "# mask = np.zeros(cont.shape[:2], dtype=\"uint8\") * 255\n",
    " \n",
    "# # Draw the contours on the mask\n",
    "# cv2.drawContours(mask, cnts, -1, (255, 255, 255), -1)\n",
    " \n",
    "# # remove the contours from the image and show the resulting images\n",
    "# img = cv2.bitwise_and(cont, cont, mask=mask)\n",
    "# cv2.imshow(\"Mask\", img)\n",
    "# cv2.waitKey(0)\n",
    " \n",
    "# for c in cnts:\n",
    "#     x, y, w, h = cv2.boundingRect(c)\n",
    "#     if w > 50 and h > 130:\n",
    "#         new_img = img[y:y + h, x:x + w]\n",
    " \n",
    "#         cv2.imwrite('Cropped.png', new_img)\n",
    "#         cv2.imshow(\"Cropped\", new_img)\n",
    "#         cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "image = cv2.imread(\"scenary.jpeg\")\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY) # grayscale\n",
    "_,thresh = cv2.threshold(gray,150,255,cv2.THRESH_BINARY_INV) \n",
    "#threshold\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3))\n",
    "dilated = cv2.dilate(thresh,kernel,iterations = 13) # dilate\n",
    "contours, hierarchy = cv2.findContours(dilated,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE) \n",
    "# get contours\n",
    "# for each contour found, draw a rectangle around it on original \n",
    "image\n",
    "for contour in contours:\n",
    " \n",
    "    # get rectangle bounding contour\n",
    " \n",
    "    [x,y,w,h] = cv2.boundingRect(contour)\n",
    "    # discard areas that are too large\n",
    " \n",
    "    if h>300 and w>300:\n",
    " \n",
    "        continue\n",
    "    # discard areas that are too small\n",
    " \n",
    "    if h<40 or w<40:\n",
    " \n",
    "        continue\n",
    "    # draw rectangle around contour on original image\n",
    " \n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,255),2)\n",
    "# write original image with added contours to disk  \n",
    " \n",
    "cv2.imwrite(\"contoured.jpg\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "/Users/jenkins/miniconda/0/2.7/conda-bld/work/opencv-2.4.11/modules/imgproc/src/color.cpp:3739: error: (-215) scn == 3 || scn == 4 in function cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6c06c86021e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sachin.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: /Users/jenkins/miniconda/0/2.7/conda-bld/work/opencv-2.4.11/modules/imgproc/src/color.cpp:3739: error: (-215) scn == 3 || scn == 4 in function cvtColor\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "img = cv2.imread('sachin.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseball\n",
    "\n",
    "img = Image.open('single_shot_detector_example.jpg')\n",
    "img.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    " \n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--image\", required=True,\n",
    "\thelp=\"path to input image\")\n",
    "ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "\thelp=\"path to Caffe 'deploy' prototxt file\")\n",
    "ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "\thelp=\"path to Caffe pre-trained model\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2,\n",
    "\thelp=\"minimum probability to filter weak detections\")\n",
    "args = vars(ap.parse_args())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the list of class labels MobileNet SSD was trained to\n",
    "# detect, then generate a set of bounding box colors for each class\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "\t\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "\t\"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "\t\"sofa\", \"train\", \"tvmonitor\"]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the input image and construct an input blob for the image\n",
    "# by resizing to a fixed 300x300 pixels and then normalizing it\n",
    "# (note: normalization is done via the authors of the MobileNet SSD\n",
    "# implementation)\n",
    "image = cv2.imread(args[\"image\"])\n",
    "(h, w) = image.shape[:2]\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 0.007843,\n",
    "\t(300, 300), 127.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pass the blob through the network and obtain the detections and\n",
    "# predictions\n",
    "print(\"[INFO] computing object detections...\")\n",
    "net.setInput(blob)\n",
    "detections = net.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop over the detections\n",
    "for i in np.arange(0, detections.shape[2]):\n",
    "\t# extract the confidence (i.e., probability) associated with the\n",
    "\t# prediction\n",
    "\tconfidence = detections[0, 0, i, 2]\n",
    " \n",
    "\t# filter out weak detections by ensuring the `confidence` is\n",
    "\t# greater than the minimum confidence\n",
    "\tif confidence > args[\"confidence\"]:\n",
    "\t\t# extract the index of the class label from the `detections`,\n",
    "\t\t# then compute the (x, y)-coordinates of the bounding box for\n",
    "\t\t# the object\n",
    "\t\tidx = int(detections[0, 0, i, 1])\n",
    "\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    " \n",
    "\t\t# display the prediction\n",
    "\t\tlabel = \"{}: {:.2f}%\".format(CLASSES[idx], confidence * 100)\n",
    "\t\tprint(\"[INFO] {}\".format(label))\n",
    "\t\tcv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "\t\t\tCOLORS[idx], 2)\n",
    "\t\ty = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "\t\tcv2.putText(image, label, (startX, y),\n",
    "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
