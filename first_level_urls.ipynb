{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from urlparse import urlparse\n",
    "import pandas as pd\n",
    "import openpyxl as op\n",
    "import os\n",
    "import urllib2, re\n",
    "import ssl\n",
    "import time\n",
    "\n",
    "\n",
    "os.chdir('/Users/krothaps/Desktop/hackathon/dataset/tim_first_level/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def fetch_text_from_url(url):\n",
    "    html = urllib2.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    data = soup.findAll(text=True)\n",
    " \n",
    "    result = filter(visible, data)\n",
    "\n",
    "    page = ' '.join(str(e.encode('utf-8')) for e in result)\n",
    "    page = page.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', '')\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_recursive_urls(univ_url, univ_name):\n",
    "    university_url = univ_url\n",
    "#     univ_name = re.sub(r'\\W+', '', univ_name)\n",
    "    university_name = univ_name.lower().strip().replace(\" \", \"_\")\n",
    "    output_file_name = university_name+'.txt'\n",
    "    first_level_urls = []\n",
    "    second_level_urls = []\n",
    "    parsed_urls = []\n",
    "    random_flag = 0\n",
    "    output_file = open(output_file_name, 'w')\n",
    "\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    html_page = urllib2.urlopen(university_url).read()\n",
    "#     html_page2 = urllib2.urlopen(university_url)\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "#     soup2 = BeautifulSoup(html_page2, \"lxml\")\n",
    "\n",
    "    \n",
    "    # fetch text from the main page\n",
    "    data = soup.findAll(text=True)\n",
    "    result = filter(visible, data)\n",
    "    page = ' '.join(str(e.encode('utf-8')) for e in result)\n",
    "    page = page.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', '')\n",
    "    parsed_urls.append(university_url)\n",
    "    \n",
    "    output_file.write(university_name+'\\t'+university_url+'\\t'+university_url+'\\t'+page+'\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    tag_data1 = soup.findAll('a', attrs={'href': re.compile(\"^http://\")})\n",
    "    tag_data2 = soup.findAll('a', attrs={'href': re.compile(\"^https://\")})\n",
    "    links1 = [link.get('href').encode('utf-8').strip() for link in tag_data1]\n",
    "    links2 = [link.get('href').encode('utf-8').strip() for link in tag_data2]\n",
    "    links = list(set(links1+links2))\n",
    "#     print links\n",
    " \n",
    "    for link in links:\n",
    "        try:\n",
    "            if link not in parsed_urls and '.edu' in link:\n",
    "                url_text = fetch_text_from_url(link)\n",
    "                output_file.write(university_name+'\\t'+university_url+'\\t'+link+'\\t'+ url_text+'\\n')\n",
    "            else:\n",
    "                url_text = 'not-parsed'\n",
    "                output_file.write(university_name+'\\t'+university_url+'\\t'+link+'\\t'+ url_text+'\\n')\n",
    "            parsed_urls.append(link)    \n",
    "        except:\n",
    "            random_flag += 1\n",
    "            \n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "\n",
    "    output_file.close()\n",
    "    print univst_name+','+univ_url+','+str(len(links))+','+str(random_flag)+','+str(int(execution_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mit_demo,http://www.mit.edu/,16,0,5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/krothaps/Desktop/hackathon/dataset/tim_list_v1.csv') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "# content = [x.strip() for x in content][0]\n",
    "# print len(content)\n",
    "# lines = content.split('\\n')\n",
    "\n",
    "print len(content)\n",
    "# print len(lines)\n",
    "\n",
    "for line in content:\n",
    "\n",
    "    if len(line.strip().split(',')) == 4:\n",
    "        univ_name = line.strip().split(',')[0]\n",
    "        university_name = str(univ_name)\n",
    "        output_file_name = str(university_name)+'.txt'\n",
    "\n",
    "        univ_url = line.strip().split(',')[2].strip()\n",
    "        if 'url_exists' == line.strip().split(',')[3] and not os.path.isfile(output_file_name):\n",
    "            try:\n",
    "                fetch_recursive_urls(univ_url, univ_name)\n",
    "            except:\n",
    "                print 'hi'\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_univ_url(line):\n",
    "    line_list = line.strip().split(',')\n",
    "    if line_list[2] != '#N/A' and '.edu' in line_list[2]:\n",
    "        return line_list[2]\n",
    "    elif 'http' in line_list[4]:\n",
    "        return line_list[4]\n",
    "    elif 'www' in line_list[4]:\n",
    "        return 'http://'+line_list[4]\n",
    "    elif 'edu' in line_list[4]:\n",
    "        return 'http://www.'+line_list[4]\n",
    "    else:\n",
    "        return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"bs4.dammit\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106412,http://www.uapb.edu,127,13,827\n",
      "175421,http://www.belhaven.edu,67,2,36\n",
      "215099,http://www.philau.edu,0,0,0\n",
      "hi\n",
      "174075,http://www.umcrookston.edu/,26,0,12\n",
      "186432,http://www.saintpeters.edu,114,0,298\n",
      "156286,http://www.bellarmine.edu,21,2,8\n",
      "hi\n",
      "133711,http://www.flagler.edu,12,1,1\n",
      "hi\n",
      "159416,http://www.lsus.edu/,11,0,1\n",
      "195234,http://www.strose.edu,111,2,273\n",
      "195173,http://www.sfc.edu,31,1,43\n",
      "hi\n",
      "482699,http://www.sgsc.edu,26,0,7\n",
      "238980,http://www.lakeland.edu,14,0,1\n",
      "173045,http://www.augsburg.edu,16,1,1\n",
      "204705,http://www.newark.osu.edu,57,3,37\n",
      "184694,http://www.fdu.edu,54,1,10\n",
      "hi\n",
      "162283,http://www.coppin.edu,39,0,18\n",
      "128391,http://www.western.edu,64,1,4\n",
      "204936,http://www.otterbein.edu,34,2,20\n",
      "139010,http://www.bainbridge.edu,2,0,1\n",
      "213987,http://www.mercyhurst.edu,85,1,421\n",
      "219347,http://www.sdsmt.edu,24,0,10\n",
      "hi\n",
      "203526,http://www.geauga.kent.edu,25,0,12\n",
      "154235,http://www.sau.edu,9,2,3\n",
      "207564,http://www.osuit.edu/,114,0,81\n",
      "hi\n",
      "104586,http://www.erau.edu,0,0,0\n",
      "203474,http://www.trumbull.kent.edu,45,0,19\n",
      "195216,http://www.stlawu.edu,37,1,22\n",
      "240426,http://www.uwsuper.edu,15,0,4\n",
      "148335,http://www.robertmorris.edu/,81,2,128\n",
      "220516,http://www.king.edu,42,11,16\n",
      "218441,http://www.ngu.edu,9,0,0\n",
      "210775,http://www.alvernia.edu,9,0,0\n",
      "hi\n",
      "228149,http://www.stmarytx.edu/,68,0,34\n",
      "237066,http://www.whitworth.edu,0,0,0\n",
      "196033,http://www.cobleskill.edu/,29,2,16\n",
      "214272,http://www.neumann.edu,42,0,12\n",
      "147013,http://www.mckendree.edu/prospective/admissions/index.aspx,17,0,11\n",
      "hi\n",
      "102632,http://www.uas.alaska.edu,83,0,111\n",
      "133021,http://www.chipola.edu,31,0,43\n",
      "hi\n",
      "233897,https://www.uvawise.edu/,66,0,81\n",
      "hi\n",
      "214069,http://www.misericordia.edu,48,5,13\n",
      "236258,http://www.pencol.edu,63,1,52\n",
      "198969,http://www.methodist.edu,42,0,369\n",
      "hi\n",
      "hi\n",
      "151786,http://www.marian.edu,28,0,4\n",
      "204185,http://www.mountunion.edu,21,1,8\n",
      "hi\n",
      "215743,http://www.francis.edu,18,1,5\n",
      "173300,http://www.cord.edu,181,0,72\n",
      "243665,http://www.uvi.edu,23,0,23\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/krothaps/Desktop/hackathon/dataset/tim_list_v2.csv','rU') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "# content = [x.strip() for x in content][0]\n",
    "# print len(content)\n",
    "# lines = content.split('\\n')\n",
    "\n",
    "# print len(content)\n",
    "# print content\n",
    "# print len(lines)\n",
    "\n",
    "for line in content:\n",
    "\n",
    "    if len(line.strip().split(',')) == 5:\n",
    "        \n",
    "        univ_name = line.strip().split(',')[0]\n",
    "        output_file_name = str(univ_name)+'.txt'\n",
    "\n",
    "        univ_url = fetch_univ_url(line)\n",
    "        if not os.path.isfile(output_file_name) and 'edu' in univ_url and univ_url != 'none':\n",
    "            try:\n",
    "                fetch_recursive_urls(univ_url, univ_name)\n",
    "            except:\n",
    "                print 'hi'\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
